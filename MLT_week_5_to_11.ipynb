{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLT_week_5_to_11.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "a7wFImHYLArp",
        "W276PS4BStSj",
        "e0Kygpak3-wD",
        "KJE6pJYn8zf1",
        "2sBvKE2K-FGB",
        "12zPzXFl62Li",
        "sKQqH5EOA6tO",
        "TDLl3meubCEV",
        "i4yNWx49g3Qz",
        "TF4S4inYxcf6",
        "zJn49vw5Iltd"
      ],
      "authorship_tag": "ABX9TyOwP6OFx4PN1gprBGchC7e2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samapatil1729/Business-Analytics/blob/main/MLT_week_5_to_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**WEEK_5**"
      ],
      "metadata": {
        "id": "XomPaehwUM2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 1\n",
        "Define a function `cross_entropy(y,sigmoid_vector,w,reg_type,reg_rate) `\n",
        "having the \n",
        "following characteristics: \n",
        "\n",
        "Input:\n",
        "\n",
        "`y`: Actual output label vector\n",
        "\n",
        "`sigmoid_vector`: logistic value of predicted output \n",
        "\n",
        "`w`: weight vector\n",
        "\n",
        "`reg_type`: type of regularization as string, either 'l1' \n",
        "or 'l2'. Default 'l2'.\n",
        "\n",
        "`reg_rate`: regularization rate. Default value 0.\n",
        "\n",
        "`Output:`\n",
        "\n",
        "Binary cross entropy loss(float value)"
      ],
      "metadata": {
        "id": "ynb5JGOFO6UD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YEJXYejRa8t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def cross_entropy(y,sigmoid_vector,w, reg_type,reg_rate):\n",
        "    y_hat = sigmoid_vector\n",
        "    base_loss = -np.sum(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))\n",
        "    if reg_type == 'l1':\n",
        "        penalty = (reg_rate/2)*np.sum(np.abs(w))\n",
        "    else:\n",
        "        penalty = (reg_rate/2)*np.sum(w*w)\n",
        "    loss = base_loss + penalty\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 2\n",
        "Write a function sigmoid(X) which returns logistic function of X, where X is a numpy array.\n",
        "\n",
        "Input: A numpy array X.\n",
        "\n",
        "Output: Logistic function of X."
      ],
      "metadata": {
        "id": "s8KXt-InapTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sigmoid(X):\n",
        "  sig = 1/(1+np.exp(-X))\n",
        "  return sig"
      ],
      "metadata": {
        "id": "USIlF__paLl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 1\n",
        "\n",
        "Assume that we have trained a logistic regression classifier on a dataset and have learned the weight `w`. Define a function `predict_label(X,w)` which accepts a feature matrix `X` of test samples and the weight vector `w` as arguments, and assigns labels to each of the samples based on the following conditions:\n",
        "\n",
        "If the model's output is greater than or equal to 0.75, assign the predicted label as 1\n",
        "\n",
        "If the model's output is less than or equal to 0.25, assign the predicted label as -1\n",
        "\n",
        "Otherwise, assign the label as 0\n",
        "\n",
        "The function should return the vector of predicted labels. Use the `sigmoid` activation function while calculating the model's output for all the sample values in the test-set."
      ],
      "metadata": {
        "id": "HOY5yOc8bZsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def predict_label(X,w):\n",
        "    z = X@w\n",
        "    sig = 1 / (1+np.exp(-z))\n",
        "    labels = (sig >= 0.25).astype(int)\n",
        "    return labels"
      ],
      "metadata": {
        "id": "LYWHgSKScAgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR"
      ],
      "metadata": {
        "id": "YYaKiK77wxU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def predict_label(x,w):  \n",
        "  y_hat = 1/(1+np.exp(-(x @ w)))\n",
        "  labels=[]\n",
        "  # compute label of the sample\n",
        "  for y in y_hat:\n",
        "    if y<=0.25:\n",
        "      label=-1\n",
        "    elif y>=0.75:\n",
        "      label=1\n",
        "    else:\n",
        "      label=0\n",
        "    labels.append(label)\n",
        "  return np.array(labels)"
      ],
      "metadata": {
        "id": "81TbXia0wwXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 2\n",
        "\n",
        "Define a function `gradient(X, y, w, reg_rate)` which can be used for optimization of `logistic regression model` with `L2` regularization having the following characteristics:\n",
        "\n",
        "`Input:`\n",
        "\n",
        "X: Feature matrix for training data.\n",
        "\n",
        "y: Label vector for training data.\n",
        "\n",
        "reg_rate: regularization rate\n",
        "\n",
        "w: weight_vector\n",
        "\n",
        "Output:\n",
        "\n",
        "A vector of gradients."
      ],
      "metadata": {
        "id": "r9l_ng6GxDFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def gradient(X,y,w,reg_rate):\n",
        "    z = X@w\n",
        "    y_hat = 1 / (1+np.exp(-z))\n",
        "    G = np.transpose(X) @ (y_hat-y) + reg_rate * w\n",
        "    return G"
      ],
      "metadata": {
        "id": "TCiWE2wfw480"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 3\n",
        "Define a function `update_w(X, y, w, reg_rate, lr)` which can be used for optimization of logistic regression model with L2 regularization having following characteristics:\n",
        "\n",
        "`Input:`\n",
        "\n",
        "`X`: Feature matrix for training data.\n",
        "\n",
        "`y`: Label vector for training data.\n",
        "\n",
        "`reg_rate`: regularization rate\n",
        "\n",
        "`w`: weight_vector\n",
        "\n",
        "`lr`: learning rate\n",
        "\n",
        "`Output`:\n",
        "\n",
        "A vector of updated weights.\n",
        "\n",
        "You need to perform exactly one update over the entire data."
      ],
      "metadata": {
        "id": "9oNS7JYC9OjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def update_w(X, y, w, reg_rate, lr):\n",
        "    z = X@w\n",
        "    sig = 1 / (1+np.exp(-z))\n",
        "    G = np.transpose(X) @ (sig - y) + reg_rate * w\n",
        "    U = w - lr*G\n",
        "    return U"
      ],
      "metadata": {
        "id": "d4eKOcQl9idi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**WEEK_6**"
      ],
      "metadata": {
        "id": "qqHljMMvOGaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 1\n",
        "\n",
        "\n",
        "\n",
        "Write a function named `bernoulli_naive_bayes` that accepts a feature matrix `X` and a label vector `y` as arguments. \n",
        "\n",
        "It should return the parameter matrix P. Both the arguments and the return value are of type np.ndarray. You can assume that no smoothing is required."
      ],
      "metadata": {
        "id": "TlHBF6kWO_Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def bernoulli_naive_bayes(X, y):\n",
        "    \"\"\"\n",
        "    Estimate the parameter matrix\n",
        "    \n",
        "    Arguments:\n",
        "    \tX: feature matrix, (100, 2), np.ndarray\n",
        "    \ty: label vector, (100, ), np.ndarray\n",
        "    Return:\n",
        "    \tP: parameter matrix, (2, 2), np.ndarray\n",
        "    \"\"\"\n",
        "    a,b = X.shape\n",
        "    classes = np.unique(y)\n",
        "    n = len(classes)\n",
        "    w = np.zeros((b,n), dtype = np.float64)\n",
        "    for i, c in enumerate(classes):\n",
        "        x_c = X[y == c]\n",
        "        w[i,:] = np.sum(x_c, axis = 0)/x_c.shape[0]\n",
        "    return w.T"
      ],
      "metadata": {
        "id": "bineFy579li1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1,0],[0,1],[0,1],[1,0]])\n",
        "y = np.array([1,0,0,1])\n",
        "bernoulli_naive_bayes(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-02LmufQGAVO",
        "outputId": "6c2e10cf-1f79-4f0f-88a5-1e54c490a7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 2\n",
        "\n",
        "You are given a numerical data matrix `x` as an `np.ndarray (shape 200 \\times 5200×5)` and a vector of class labels y `(size 200)` as np.ndarray for a multi-class classification problem. \n",
        "\n",
        "Define a function `mean_estimate` which calculates the `estimated mean` of data samples corresponding to the class labels for each feature and returns a dictionary with class labels as keys and estimated mean vectors as values. The i^{th}i \n",
        "th\n",
        "  element of a mean vector corresponds to the i^{th}i \n",
        "th\n",
        "  feature.\n"
      ],
      "metadata": {
        "id": "a7wFImHYLArp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def mean_estimate(X: np.ndarray,  y : np.ndarray):\n",
        "    \"\"\"\n",
        "    Estimate the mean of samples for each class\n",
        "\n",
        "    Arguments:\n",
        "        X: samples, (200, 5), np.ndarray\n",
        "        y: labels, (200, ), np.ndarray\n",
        "    Return:\n",
        "        D: dictionary\n",
        "            key: label, int\n",
        "            value: mean, np.ndarray\n",
        "    \"\"\"\n",
        "    a,b = X.shape\n",
        "    classes = np.unique(y)\n",
        "    d = {}\n",
        "    for c in classes:\n",
        "        x_c = X[y == c]\n",
        "        d[c] = np.mean(x_c, axis = 0)\n",
        "    return d"
      ],
      "metadata": {
        "id": "9FTSMjV9KtgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 3\n",
        "Write a function `naive_gaussian_predict` that implements a `Gaussian Naive Bayes` model on training data, and returns the predicted class labels for the test data. \n",
        "\n",
        "This is a binary classification task `(labels are 0 and 1)` and the size of `X_train, y_train` and `X_test` are fixed to \n",
        "\n",
        "(800×2), (800, ) and (200×2) respectively. The function signature is as follows:\n",
        "\n",
        "    Arguments:\n",
        "        X_train: train samples, (800, 2), np.ndarray \n",
        "        y_train: train labels,  (800, ), np.ndarray \n",
        "        X_test:  test samples, (200, 2), np.ndarray\n",
        "    Return  \n",
        "        y_pred: test labels, (200, ) np.ndarray"
      ],
      "metadata": {
        "id": "W276PS4BStSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def fit(X,y):\n",
        "    n_samples, n_features= X.shape\n",
        "    classes= np.unique(y)\n",
        "    n_classes= len(classes)\n",
        "    mean= np.zeros((n_classes, n_features), dtype= np.float64)\n",
        "    var= np.zeros((n_classes, n_features), dtype= np.float64)\n",
        "    priors= np.zeros(n_classes, dtype= np.float64)\n",
        "    for i, c in enumerate(classes):\n",
        "        X_c= X[y==c]\n",
        "        mean[i,:]= X_c.mean(axis=0)\n",
        "        var[i,:]= X_c.var(axis=0)\n",
        "        priors[i]= X_c.shape[0]/float(n_samples)\n",
        "    return (mean, var, priors, classes)\n",
        "def calc_pdf(class_id, X,mean,var):\n",
        "    mean= mean[class_id]\n",
        "    var= np.diag(var[class_id])\n",
        "    z= np.power(2* np.pi,X.shape[0]/2) *np.power(np.linalg.det(var),1/2)\n",
        "    return (1/z)* np.exp(-(1/2)*(X-mean).T @(np.linalg.inv(var))@(X-mean))\n",
        "def calc_prod_likelihood_prior(X, classes, priors, mean, var):\n",
        "    prod_likelihood_prior= np.zeros((X.shape[0], len(classes)), dtype= np.float64)\n",
        "    for x_i, x in enumerate(X):\n",
        "        for i, c in enumerate(classes):\n",
        "            prod_likelihood_prior[x_i,c]= (np.log(calc_pdf(i,x, mean, var))+ np.log(priors[i]))\n",
        "    return prod_likelihood_prior\n",
        "def naive_gaussian_predict(X_train, y_train, X_test):\n",
        "    mean, var, priors, classes= fit(X_train,y_train)\n",
        "    likelihood_prior= calc_prod_likelihood_prior(X_test, classes, priors, mean, var)\n",
        "    return np.argmax(likelihood_prior, axis=1)\n",
        "    \"\"\"\n",
        "    Train a Gaussian NB and predict the labels on test set \n",
        "\n",
        "    Arguments:\n",
        "        X_train: train samples, (800, 2), np.ndarray \n",
        "        y_train: train labels,  (800, ), np.ndarray \n",
        "        X_test:  test samples, (200, 2), np.ndarray\n",
        "    Return  \n",
        "        y_pred: test labels, (200, ) np.ndarray\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "ErRhD7zKTC88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 4\n",
        "\n",
        "For a binary classification problem with class labels (0 and 1), define a function `class_scores` that accepts the true and predicted labels and returns the following evaluation metrics as a dictionary.\n",
        "\n",
        "`(1) Precision`\n",
        "\n",
        "`(2) Recall`\n",
        "\n",
        "`(3) Accuracy`\n",
        "\n",
        "`(4) F1 score`\n",
        "\n",
        "`(5) Misclassification Rate`\n",
        "\n",
        "They keys of the dictionary are the names of the metrics, exactly as they are given above. The values are the corresponding measurements expressed as floats. The function should have the following signature:\n",
        "Arguments:  \n",
        "\n",
        "    y_test: true labels, (n, ), np.ndarray \n",
        "    y_pred: predicted labels, (n, ), np.ndarray\n",
        "Return:\n",
        "\n",
        "    metrics: dictionary\n",
        "        key: string, names of the metrics\n",
        "        value: float\n",
        "\n",
        "Both numpy arrays are of size (n, ). Do not use any existing methods/functions to calculate the same. Consider label 1 as the positive class. Note that the misclassification rate is 1 minus the accuracy."
      ],
      "metadata": {
        "id": "e0Kygpak3-wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def class_scores(y_test: np.ndarray, y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics for a binary classification task\n",
        "    \n",
        "    Arguments:  \n",
        "        y_test: true labels, (n, ), np.ndarray \n",
        "        y_pred: predicted labels, (n, ), np.ndarray\n",
        "    Return:\n",
        "        metrics: dictionary\n",
        "            key: string, names of the metrics\n",
        "            value: float\n",
        "    \"\"\"\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for i in range(len(y_test)):\n",
        "        if y_test[i] == 1 and y_pred[i] == 1:\n",
        "            TP+=1\n",
        "        elif y_test[i] == 0 and y_pred[i] == 0:\n",
        "            TN+=1\n",
        "        elif y_test[i] == 1 and y_pred[i] == 0:\n",
        "            FN+=1\n",
        "        else:\n",
        "            FP+=1\n",
        "            \n",
        "    P = TP/(TP+FP)\n",
        "    R = TP/(TP+FN)\n",
        "    A = (TP+TN)/(TP+TN+FN+FP)\n",
        "    f1 = (2*P*R)/(P+R)\n",
        "    MCR = (FN+FP)/(TP+TN+FN+FP)\n",
        "    \n",
        "    d = {'Precision':P, \"Recall\":R, 'Accuracy':A, 'F1_score':f1, 'Misclassification_rate':MCR}\n",
        "    return d"
      ],
      "metadata": {
        "id": "1yj3lwoP4fnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 1\n",
        "In a multi-class classification setting, consider a numerical feature matrix `X` as an np.ndarray of shape (n, m) and a vector of class labels `y` of size (n, ) as an np.ndarray. \n",
        "\n",
        "Define a function `variance_estimate` which calculates the estimated variance of data samples corresponding to individual class labels for each feature. The function should return a dictionary with class labels as keys and estimated variance vectors as values. The i^{th} element of a vector corresponds to the variance of the i^{th} feature."
      ],
      "metadata": {
        "id": "KJE6pJYn8zf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def variance_estimate(X: np.ndarray, y: np.ndarray):\n",
        "    \n",
        "    \"\"\"\n",
        "    Estimate the variance of samples for each class\n",
        "\n",
        "    Arguments:\n",
        "        X: feature matrix, (n, m), np.ndarray\n",
        "        y: label vector, (n, ), np.ndarray\n",
        "    Return:\n",
        "        D: dictionary\n",
        "           key: label, int\n",
        "           value: variance, (m, ), np.ndarray\n",
        "    \"\"\"\n",
        "    class_count = np.unique(y)\n",
        "    class_dic = {}\n",
        "    for c in (class_count):\n",
        "        X_c = X[y==c]\n",
        "        class_dic[c] = X_c.var(axis = 0)\n",
        "    return class_dic"
      ],
      "metadata": {
        "id": "6evpr31I9Zr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR"
      ],
      "metadata": {
        "id": "AZHWAFOy99SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def variance_estimate(X: np.ndarray,  y: np.ndarray):\n",
        "    D = {}\n",
        "    labels = np.unique(y)\n",
        "    for label in labels:\n",
        "        D[label] = np.var(X[y == label], axis = 0)\n",
        "    return D"
      ],
      "metadata": {
        "id": "Sizynr_J92St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 2\n",
        "Write a function `naive_gmodel_eval` that implements a Gaussian Naive Bayes model on training data, predicts class labels for the test data and returns the evaluation scores for the predictions performed on the test data, corresponding to `each individual label `appearing on `y_test`. \n",
        "\n",
        "This is a multiclass classification task and the size of `X_train, y_train, X_test and y_test` are fixed to (1800, 5), (1800,), (200,5) and (200,) respectively. There are four classes labeled as 0, 1, 2, 3\n",
        "\n",
        "\n",
        "---\n",
        "For each label, the following evaluation metrics have to be generated by treating that label as a positive class and all others as the negative class.\n",
        "\n",
        "(1) Precision\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Accuracy\n",
        "\n",
        "(4) F1 score\n",
        "\n",
        "(5) Misclassification Rate\n",
        "\n",
        "This information has to be stored in a dictionary. They keys of the dictionary are the names of the metrics, exactly as they are given above. The values are the corresponding measurements expressed as floats.\n",
        "\n",
        "\n",
        "---\n",
        "Create a parent dictionary named `metrics`. The keys of this dictionary will be the labels and the values will be the corresponding evaluation dictionaries. So, your function should return `metrics`, which is essentially a dictionary of dictionaries!\n",
        "\n",
        "\n",
        "---\n",
        "The function will have the following signature:\n",
        "\n",
        "    Arguments:\n",
        "        X_train: training samples, (1800, 5), np.ndarray\n",
        "        y_train: training labels, (1800, ), np.ndarray, labels are 0, 1, 2 or 3\n",
        "        X_test:  test samples, (200, 5), np.ndarray\n",
        "        y_test:  test_labels, (200, ), np.ndarray, labels are 0, 1, 2, or 3\n",
        "    Return:\n",
        "        metrics: dict of dicts\n",
        "           key: label, int\n",
        "           value: dict\n",
        "               key: string\n",
        "               value: float\n",
        "\n"
      ],
      "metadata": {
        "id": "2sBvKE2K-FGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def class_scores(y_test, y_pred, positive):\n",
        "    tp = np.sum((y_test == positive) & (y_pred == positive))\n",
        "    tn = np.sum((y_test != positive) & (y_pred != positive))\n",
        "    fp = np.sum((y_test != positive) & (y_pred == positive))\n",
        "    fn = np.sum((y_test == positive) & (y_pred != positive))\n",
        "\n",
        "    names = (\"Precision\", \"Recall\",\"Accuracy\", \"Misclassification Rate\",\"F1 score\")\n",
        "    values = [tp / (tp + fp), tp / (tp + fn), (tp + tn) / (tp + tn + fp + fn)]\n",
        "    values.append(1 - values[-1])\n",
        "    values.append(2 * values[0] * values[1] / (values[0] + values[1]))\n",
        "\n",
        "    scores = dict(zip(names,values))\n",
        "    return scores\n",
        "\n",
        "# Solution\n",
        "class NB(object):\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        # calculate mean, var, and prior for each class\n",
        "        self._mean = np.zeros((n_classes, n_features), dtype = np.float64)\n",
        "        self._var = np.zeros((n_classes, n_features), dtype = np.float64)\n",
        "        self._priors = np.zeros(n_classes, dtype = np.float64)\n",
        "\n",
        "        for c in self._classes:\n",
        "            X_c = X[y == c]\n",
        "            self._mean[c, :] = X_c.mean(axis = 0)\n",
        "            self._var[c, :] = X_c.var(axis = 0)\n",
        "            self._priors[c] = X_c.shape[0] / float(n_samples)\n",
        "\n",
        "    def predict(self, X):\n",
        "        self._posterior = np.zeros((X.shape[0], len(self._classes)), dtype = np.float64)\n",
        "        for idx, x in enumerate(X):\n",
        "            for c in self._classes:\n",
        "                self._posterior[idx, c] = np.log(self._pdf(c, x)) + np.log(self._priors[c])\n",
        "        return np.argmax(self._posterior, axis = 1)\n",
        "\n",
        "    def _pdf(self, class_idx, X):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = np.diag(self._var[class_idx])\n",
        "        z = np.power(2 * np.pi, X.shape[0]/2) * np.power(np.linalg.det(var), 1/2)\n",
        "        return (1 / z) * np.exp(-(1 / 2) * (X - mean).T @ (np.linalg.inv(var)) @ (X - mean))\n",
        "\n",
        "def naive_gmodel_eval(X_train, y_train, X_test, y_test):\n",
        "    gaussian_nb = NB()\n",
        "    gaussian_nb.fit(X_train, y_train)\n",
        "    y_pred = gaussian_nb.predict(X_test)\n",
        "\n",
        "    labels = np.unique(y_train)\n",
        "    D = {}\n",
        "    for label in labels:\n",
        "        D[label] = class_scores(y_test, y_pred, label)\n",
        "    return D"
      ],
      "metadata": {
        "id": "2V-61KK3-yUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**WEEK_7**"
      ],
      "metadata": {
        "id": "_73SsMyOqUMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 1\n",
        "Write a function `euclid(a,b)` to find `Euclidean distance` between vectors a and b. Both a and b have shape (n,1), where n is number of features/dimensions.\n",
        "\n",
        "Input: Vectors a and b\n",
        "\n",
        "Output: Euclidean distance between vectors a and b"
      ],
      "metadata": {
        "id": "12zPzXFl62Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def euclid(a,b):\n",
        "  E = np.sum((a-b)**2,axis=1)\n",
        "  return E"
      ],
      "metadata": {
        "id": "VlamwA5V_k_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 2\n",
        "\n",
        "Write a function `one_hot(y)` which performs `one hot encoding` on vector y and then outputs a resultant matrix which can be used for softmax regression as output label matrix. y is row matrix with (n,1) shape, where n is number of samples.\n",
        "\n",
        "Example: if y is `[8, 6, 3]`, its one hot encoding will be\n",
        " [[0, 0, 1],[0, 1, 0],[1, 0, 0]].\n",
        "\n",
        "Input: y: A vector of shape (n,1)\n",
        "\n",
        "Output: A output label matrix of suitable shape."
      ],
      "metadata": {
        "id": "sKQqH5EOA6tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def one_hot(y):\n",
        "    label_list = list(np.unique(y))\n",
        "    M = np.zeros((len(y),len(label_list)))\n",
        "    for i in range(len(y)):\n",
        "        M[i,label_list.index(y[i])] = 1       # M[i][label_list.index(y[i])] = 1\n",
        "    return M"
      ],
      "metadata": {
        "id": "OY913O4YW39W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = [8, 6, 3]"
      ],
      "metadata": {
        "id": "8t_bHsbfBfi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK2u93MmUAkW",
        "outputId": "30429d85-e3cd-4db6-822c-0cc320d508b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye6xhhZnZuG2",
        "outputId": "30ac5dee-19b8-49cf-b4c3-dbc380816a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDPylmCPaE-Y",
        "outputId": "ef96c422-f919-4db3-dce8-0049b2c21d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 6, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6g7vYjYLtkcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 1\n",
        "\n",
        "Write a function `manhattan(a,b)` to find `Manhattan distance` between vectors a and b. Both a and b are vectors with shape (n,1) where n is number of features/dimensions.\n",
        "\n",
        "Input: Vectors a and b\n",
        "\n",
        "Output: Manhattan distance between vectors a and b"
      ],
      "metadata": {
        "id": "TDLl3meubCEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def manhattan(a,b):\n",
        "  dist = np.sum(np.abs(a-b))\n",
        "  return dist"
      ],
      "metadata": {
        "id": "dIFyfpqbbBdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1, 2, 3])\n",
        "y = np.array([8, 6, 3])\n",
        "\n",
        "manhattan(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SehQcN5Zbc0j",
        "outputId": "1b273d49-41a6-490e-df80-858441f45ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 2\n",
        "Write a `function softmax(Z)` to find softmax of linear combination of feature matrix and weight vector.\n",
        "\n",
        "Take care of numerical stability as well.\n",
        "\n",
        "`Input`: Z = X@W, where X is feature matrix of shape (n,m), W is weight matrix (m,k),\n",
        "\n",
        " where n = number of rows, m = number of features and k = number of labels.\n",
        "\n",
        "`Output`: A matrix of (n,1) shape. Each row corresponds to the label of that row. \n",
        "\n",
        "Each label will have value from 0 to k-1.\n"
      ],
      "metadata": {
        "id": "i4yNWx49g3Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def softmax(Z):\n",
        "  exp = np.exp(Z-np.max(Z))\n",
        "  for i in range (len(Z)):\n",
        "      exp[i] /= np.sum(exp[i])\n",
        "  return exp"
      ],
      "metadata": {
        "id": "o5qfTL2XhF5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z = [3.0, 1.0, 0.2]\n",
        "print(softmax(Z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkGIJJXHtpXz",
        "outputId": "f7fe305e-6baa-43b9-8453-69a62af48b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ],
      "metadata": {
        "id": "JYXGmw0jvNnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z = [3.0, 1.0, 0.2]\n",
        "print(softmax(Z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMkHcK39vPsC",
        "outputId": "df013bad-2eb2-4034-eb3f-3739c273c64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8360188  0.11314284 0.05083836]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 3\n",
        "Write a function `knn(class1, class2, x_new)` to find in which cluster x_new belongs using 3-NN. Assume cluster 1 and cluster 2 has 5 points each. Use Euclidian distance as distance measure.\n",
        "\n",
        "`Input: `\n",
        "\n",
        "Two numpy arrays class1 and class2 of shape (5,2) each and a numpy array x_new of shape (2,1)\n",
        "\n",
        "`Output:`\n",
        "\n",
        " Return class id to which x_new belongs (i.e. 1 or 2) ."
      ],
      "metadata": {
        "id": "TF4S4inYxcf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def knn(class1,class2,x_new):\n",
        "  dist1 = np.sum((np.transpose(x_new)-class1)**2,axis=1)  \n",
        "  dist2 = np.sum((np.transpose(x_new)-class2)**2,axis=1)\n",
        "  dist1 = np.append([dist1],np.ones((1,5)),axis=0)\n",
        "  dist2 = np.append([dist2],2*np.ones((1,5)),axis=0)\n",
        "  dist  = np.concatenate((dist1, dist2), axis=1)\n",
        "  #ii = np.argsort(dist[0])\n",
        "  dist  = np.sort(dist, axis=1)\n",
        "  #dist = dist[:,ii]\n",
        "  if np.sum(dist[1][0:3])<=4:\n",
        "    return 1\n",
        "  else:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "isc6Re-txcFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class1 = np.array([[1,1], [1,2], [2,3], [1,5], [3, 2]])\n",
        "class2 = np.array([[-1,-1],[-1,-2],[-2,--3], [-1,-5], [-3,-2]])\n",
        "class1.shape, class2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCDg_2-Q1vON",
        "outputId": "a1efc2b9-1edd-4b42-a1b1-695d6fe333d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5, 2), (5, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = np.array([[2], [3]])\n",
        "x_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqBn9bhb2bEd",
        "outputId": "13e0ec2a-f212-4bdb-a948-e41ca51a38b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new.T - class1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhr6W15k2uTg",
        "outputId": "b75b9d73-0abf-4b00-b588-6dc22f9727bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2],\n",
              "       [ 1,  1],\n",
              "       [ 0,  0],\n",
              "       [ 1, -2],\n",
              "       [-1,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " dist1 = np.sum((np.transpose(x_new)-class1)**2,axis=1) \n",
        " print(dist1)\n",
        " dist1 = np.append([dist1],np.ones((1,5)),axis=0)\n",
        " dist1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx7SXEW33XmM",
        "outputId": "6b456ccc-bbef-441e-8124-6da0c3609e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 2 0 5 2]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5., 2., 0., 5., 2.],\n",
              "       [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dist2 = np.sum((np.transpose(x_new)-class2)**2,axis=1)\n",
        "print(dist2)\n",
        "dist2 = np.append([dist2],2*np.ones((1,5)),axis=0)\n",
        "dist2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GCKdDc55qz2",
        "outputId": "5c1825b7-bb59-4d4d-ffd6-a8044f9b1c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25 34 16 73 50]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[25., 34., 16., 73., 50.],\n",
              "       [ 2.,  2.,  2.,  2.,  2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dist  = np.concatenate((dist1, dist2), axis=1)\n",
        "dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoIVGDTQ50I1",
        "outputId": "7ce918ef-e61e-4c9b-a22f-c15452c2cac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5.,  2.,  0.,  5.,  2., 25., 34., 16., 73., 50.],\n",
              "       [ 1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dist  = np.sort(dist, axis=1)\n",
        "dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22KCmJYl6Qeo",
        "outputId": "d67182b2-7b5d-4018-ed28-256529e99391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  2.,  2.,  5.,  5., 16., 25., 34., 50., 73.],\n",
              "       [ 1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(dist[1][0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxgh5CyE7rph",
        "outputId": "d0a7a824-55d4-4fde-e6b9-21516c0575cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "knn(class1,class2,x_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB_vSRd-8thq",
        "outputId": "bc52274a-7b6f-4281-d356-84698e374d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**WEEK_8**"
      ],
      "metadata": {
        "id": "5U93_lVS-jX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 1\n",
        "Write a function named `\"hinge_loss\"` that computes the hinge loss value for corresponding elements of two vectors namely \n",
        "`y_test` (5 x 1) and `y_pred` (5 x 1) and returns the mean of the loss values computed.\n",
        "\n",
        "Note: `y_test` is the true test label and y_pred is the the predicted label."
      ],
      "metadata": {
        "id": "whIDpaQs_jvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def hinge_loss(y_pred,y_test):\n",
        "    loss= np.mean([max(0, (1-x*y)) for x,y in zip(y_pred,y_test)])\n",
        "    return loss"
      ],
      "metadata": {
        "id": "S0pyoU4Z-mz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def hinge_loss(y_pred, y_test):\n",
        "  loss = np.mean([max(0, (1-x*y)) for x,y in zip(y_pred, y_test)])"
      ],
      "metadata": {
        "id": "3SYUuegvs4UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SWI"
      ],
      "metadata": {
        "id": "3KFQfKiHBRKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(X, y, w, b, C):\n",
        "  lin_comb = y * (X @ w +b)\n",
        "  margin = w.T @ w/2\n",
        "  hinge = C*np.sum(np.max([0, (1 - lin_comb)]))\n",
        "  return margin + hinge"
      ],
      "metadata": {
        "id": "9wuWqxP-BQqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 2\n",
        "Write a function `'solve_eqn'` to obtain the weight vector (bias as its last element) of linear SVM model by accepting \n",
        "an array of support vectors `A` of shape (3 x 2) and their label vector b of shape (3 x 1). \n",
        "This function should return weight vector of shape 3 x 1.  "
      ],
      "metadata": {
        "id": "EBp9Ie6YBwPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "CjmhJL5WKEsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def solve_eqn(A,b):\n",
        "    A = np.column_stack((A, np.ones(A.shape[0])))\n",
        "    S = [[i @ j for j in A] for i in A]\n",
        "    x = np.linalg.solve(S, b)\n",
        "    w = np.zeros_like(x)\n",
        "    for i in range(len(x)):\n",
        "        w += x[i] * A[i]\n",
        "    return w"
      ],
      "metadata": {
        "id": "lVcDMzzUIbW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1, 0], [3, 1], [3, -1]])\n",
        "b = np.array([-1, 1, 1])\n",
        "solve_eqn(A,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LLR1VZFH0rz",
        "outputId": "37c8f230-d1a7-47dc-a9b4-44982402d394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.,  0., -2.])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[8, 3], [-4, 7], [3, 4]])\n",
        "b = np.array([1, -1, 1])\n",
        "\n",
        "#print(solve_eqn(A, b))"
      ],
      "metadata": {
        "id": "Hmk-gNNWIf64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.column_stack((A, np.ones(A.shape[0])))\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8Sfa7PZ3gG6",
        "outputId": "15d29644-7840-416e-e4ff-217c1cf64b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8.,  3.,  1.],\n",
              "       [-4.,  7.,  1.],\n",
              "       [ 3.,  4.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S = [[i @ j for j in A] for i in A]\n",
        "S"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_L37mxF42Fv",
        "outputId": "7dd1b42c-c204-4ce8-89af-ecdadde05bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[74.0, -10.0, 37.0], [-10.0, 66.0, 17.0], [37.0, 17.0, 26.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in A:\n",
        "  #print(i)\n",
        "  for j in A:\n",
        "    #print(j)\n",
        "    print(i, j, i*j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nhl-0VvJLhh",
        "outputId": "b9a2b808-42b4-43c6-e766-ebf4e333398e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8. 3. 1.] [8. 3. 1.] [64.  9.  1.]\n",
            "[8. 3. 1.] [-4.  7.  1.] [-32.  21.   1.]\n",
            "[8. 3. 1.] [3. 4. 1.] [24. 12.  1.]\n",
            "[-4.  7.  1.] [8. 3. 1.] [-32.  21.   1.]\n",
            "[-4.  7.  1.] [-4.  7.  1.] [16. 49.  1.]\n",
            "[-4.  7.  1.] [3. 4. 1.] [-12.  28.   1.]\n",
            "[3. 4. 1.] [8. 3. 1.] [24. 12.  1.]\n",
            "[3. 4. 1.] [-4.  7.  1.] [-12.  28.   1.]\n",
            "[3. 4. 1.] [3. 4. 1.] [ 9. 16.  1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linalg.solve(S, b)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvGgkQVILR7t",
        "outputId": "8a5a2084-15b8-41ef-a713-81cb2e2e0a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-32.40625, -20.21875,  59.375  ])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.zeros_like(x)\n",
        "w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELJ8iq0yMG_r",
        "outputId": "aacef91b-8482-42f5-8a01-037f6c679aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(x)):\n",
        "        w += x[i] * A[i]\n",
        "        print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJdgljRLMPVq",
        "outputId": "cf772538-3693-4922-fa6e-26ba37e22e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-259.75     -99.71875  -18.90625]\n",
            "[-178.875 -241.25   -39.125]\n",
            "[-0.75 -3.75 20.25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 3\n",
        "Write a class named `'fit_softsvm'` that implements soft margin SVM using GD. \n",
        "\n",
        "Write a separate function named `'support_vectors'` and return the support_vectors identified by the model. \n",
        "\n",
        "The inputs to the `'support_vectors'` function should be feature matrix X_train and label vector y_train.\n",
        "\n",
        "Use the following parameters:\n",
        "\n",
        "1.   learning rate=0.001\n",
        "2.   C=500\n",
        "3.   epochs=100"
      ],
      "metadata": {
        "id": "zJn49vw5Iltd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class fit_softSVM:\n",
        "    def __init__(self, C=500):\n",
        "       self.support_vectors= None\n",
        "       self.C= C\n",
        "       self.W = None\n",
        "       self.b = None\n",
        "       self.X = None\n",
        "       self.y = None\n",
        "       self.n=0\n",
        "       self.d=0\n",
        "      \n",
        "    def __decision_function(self, X):\n",
        "        return X.dot(self.w) + self.b\n",
        " \n",
        "    def __cost(self, margin):\n",
        "        return 0.5 * self.w.dot(self.w) + self.C * np.sum(np.maximum(0, 1 - margin))\n",
        " \n",
        "    def __margin(self, X, y):\n",
        "        return y * self.__decision_function(X)\n",
        " \n",
        "    def fit(self, X, y, lr=0.001, epochs=100):\n",
        "        self.n, self.d = X.shape\n",
        "        self.w = np.random.randn(self.d)\n",
        "        self.b = 0\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        losses = []\n",
        "        for _ in range(epochs):\n",
        "            margin = self.__margin(X, y)\n",
        "            loss = self.__cost(margin)\n",
        "            losses.append(loss)\n",
        "            \n",
        "            misclassified_pts_idx = np.where(margin < 1)[0]\n",
        "            d_w = self.w - self.C * y[misclassified_pts_idx].dot(X[misclassified_pts_idx])\n",
        "            self.w = self.w - lr * d_w\n",
        "            \n",
        "            d_b = - self.C * np.sum(y[misclassified_pts_idx])\n",
        "            self.b = self.b - lr * d_b\n",
        "            \n",
        "        self.support_vectors = np.where(self._margin <= 1)[0]\n",
        " \n",
        "    def support_vectors(self, X_train, y_train):\n",
        "        sp= fit(X_train, y_train)\n",
        "        return sp"
      ],
      "metadata": {
        "id": "2mQin0cAIrnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 1\n",
        "Write a function named` \"hinge_loss\" `that computes the hinge loss value for two vectors \n",
        "namely y_test (20 x 1) and y_pred (20 x 1) , and returns the mean of the loss values computed. \n",
        "\n",
        "Note: y_test is the true test label and y_pred is the the predicted label."
      ],
      "metadata": {
        "id": "2QCWO8nweFp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def hinge_loss(y_pred, y_test):\n",
        "  loss = np.mean([max(0, (1 - x*y)) for x,y in zip(y_pred, y_test)])\n",
        "  return loss"
      ],
      "metadata": {
        "id": "LuifV7Y5eOib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 2\n",
        "Write a function `'fit'` to compute gradient of the hinge loss function without regularization. \n",
        "This fit function should have the following input arguments-\n",
        "\n",
        "```\n",
        "X_train- feature matrix for training\n",
        "Y_train - labels for training\n",
        "X_test - features for testing\n",
        "Y_test - labels for testing\n",
        "n_iters - Number of iterations with default value 100\n",
        "lr - learning rate with default value 0.1\n",
        "```\n",
        "\n",
        "\n",
        "The function should return the accuracy using the testing data."
      ],
      "metadata": {
        "id": "7kNn08veeyLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def decision_function(X,w,b):\n",
        "    return X.dot(w) + b\n",
        "def cost(w,C, margin):\n",
        "    return (1/2) * w.dot(w) + C * np.sum(np.maximum(0,1-margin))\n",
        "def margin(X, y,w,b):\n",
        "    return y * decision_function(X,w,b)\n",
        "def fit(X_train,Y_train,X_test,Y_test,n_iters=100,lr=0.1):\n",
        "        n, d = X_train.shape\n",
        "        w = np.random.randn(d)\n",
        "        b = 0\n",
        "        C = 1\n",
        "        \n",
        "        X = X_train\n",
        "        y = Y_train\n",
        "        loss_array = []\n",
        "        for _ in range(n_iters):\n",
        "            Margin = margin(X,y,w,b)\n",
        "            loss = cost(w,C,Margin)\n",
        "            loss_array.append(loss)\n",
        "            misclassified_pts_idx = np.where(Margin < 1)[0]\n",
        "            d_w = w - C * y[misclassified_pts_idx].dot(X[misclassified_pts_idx])\n",
        "            w = w - lr*d_w\n",
        "            d_b =- C* np.sum(y[misclassified_pts_idx])\n",
        "            b = b - lr* d_b\n",
        "            \n",
        "        support_vectors = np.where(margin(X,y,w,b) <= 1)[0]\n",
        "        y_hat = np.sign(decision_function(X_test,w,b))\n",
        "        accuracy = np.mean(Y_test == y_hat)\n",
        "        return accuracy\n"
      ],
      "metadata": {
        "id": "d9rdhyjHfEml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 3\n",
        "Write a class named `'fit_softsvm'` that implements soft margin SVM using GD. \n",
        "Write a separate function named `'compute_accuracy'` and return the accuracy value \n",
        "using a `'pred_accuracy'` function which is defined inside the 'fit_svm' class. \n",
        "\n",
        "The inputs to the `'compute_accuracy'` function should be \n",
        "feature matrices `X_train, X_test` and label vectors `y_train, y_test`.\n",
        "\n",
        "Use the following parametes:\n",
        "\n",
        "```\n",
        "1.   learning rate=0.01\n",
        "2.   C=15\n",
        "3.   epochs=100\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "dPXo1hcxgV7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class fit_softSVM:\n",
        "    def __init__(self, C=15):\n",
        "        self.support_vectors = None\n",
        "        self.C = C\n",
        "        self.b = None\n",
        "      \n",
        "    def __decision_function(self, X):\n",
        "        return X.dot(self.w) + self.b\n",
        " \n",
        "    def __cost(self, margin):\n",
        "        return 0.5 * self.w.dot(self.w) + self.C * np.sum(np.maximum(0, 1 - margin))\n",
        " \n",
        "    def __margin(self, X, y):\n",
        "        return y * self.__decision_function(X)\n",
        " \n",
        "    def fit(self, X, y, lr=0.01, epochs=100):\n",
        "        n, d = X.shape\n",
        "        self.w = np.random.randn(d)\n",
        "        self.b = 0\n",
        "        losses = []\n",
        "        for _ in range(epochs):\n",
        "            margin = self.__margin(X, y)\n",
        "            loss = self.__cost(margin)\n",
        "            losses.append(loss)\n",
        "            \n",
        "            misclassified_pts_idx = np.where(margin < 1)[0]\n",
        "            d_w = self.w - self.C * y[misclassified_pts_idx].dot(X[misclassified_pts_idx])\n",
        "            self.w = self.w - lr * d_w\n",
        "            \n",
        "            d_b = - self.C * np.sum(y[misclassified_pts_idx])\n",
        "            self.b = self.b - lr * d_b\n",
        "            \n",
        "        self._support_vectors = np.where(margin <= 1)[0]\n",
        " \n",
        "    def predict(self, X):\n",
        "        return np.sign(self.__decision_function(X))\n",
        "        \n",
        "    def score(self, X, y):\n",
        "        p = self.predict(X)\n",
        "        return np.mean(y == P)\n",
        "from sklearn.metrics import accuracy_score\n",
        "def compute_accuracy(X_train, y_train, X_test,  y_test):\n",
        "    svm = fit_softSVM(C=1)\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "    \n"
      ],
      "metadata": {
        "id": "qcYEsRQtfRXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**WEEK_9**"
      ],
      "metadata": {
        "id": "SnY3HeNdhhoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 1\n",
        "Consider a regression problem with feature matrix X with size (100, 10) and label vector y with size (100, 1). We split this root node into two nodes 'node1' and 'node2' according to jth split variable and split value 's'.\n",
        "Define a function `predict_node(X, y, j, s)` that takes X, y, split variable `j` and split value `s` as parameters and returns the tuple of predict values (mean value) at both the nodes.\n",
        "\n",
        "X = ndarray of size (100, 10) with entries as float.\n",
        "\n",
        "y = ndarray of size (100, 1) with entries as float.\n",
        "\n",
        "j = int\n",
        "\n",
        "s = float"
      ],
      "metadata": {
        "id": "sQ-a91yWhltf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def predict_node(X, y, j, s):\n",
        "    node1 = [np.where(X[:,j]<=s)]\n",
        "    node2 = [np.where(X[:,j]>s)]\n",
        "    return (np.mean(y[node1]), np.mean(y[node2]))\n",
        "    pass"
      ],
      "metadata": {
        "id": "7KEJIuqtqD5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 2\n",
        "Define a function `predict_class(y)` that takes the parameter\n",
        "`y` which is a ndarray of actual outputs of all the samples in a particualar node and retruns the predict class for the same node.\n",
        "\n",
        "Note: number of classes are 10 (0 to 9). \n",
        "\n",
        "If two classes have same number of samples, function should return the lower class. "
      ],
      "metadata": {
        "id": "pfOf7uhIqSyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def predict_class(y):\n",
        "  return np.bincount(y).argmax()"
      ],
      "metadata": {
        "id": "bWhPFGhjkeGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = [5, 8, 9, 5, 0 ,4, 0, 3, 9,2, 0, 4, 9, 2, 7, 7, 9, 8, 6, 9, 3, 7, 7, 4, 5, 9, 3, 6, 8, 0, 7]\n",
        " "
      ],
      "metadata": {
        "id": "ylwwvgFlsnnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.bincount(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4INu-QMtP3w",
        "outputId": "a0407241-44fc-46ee-a7a8-e4451e903320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 0, 2, 3, 3, 3, 2, 5, 3, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_class(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV7pt76ZrA9j",
        "outputId": "2b103db8-42e2-4553-b51a-ad00290f49e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 3\n",
        "Define a function `misclassification_error(y)`  that takes the parameter `y` which is a ndarray of actual outputs of all the samples in a particular node and return the misclassification error of the same node."
      ],
      "metadata": {
        "id": "D0q7jDf22fxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "def misclassification_error(y):\n",
        "  pred_class = np.bincount(y).argmax()\n",
        "  mis_class  = np.where(y != pred_class)\n",
        "  error = len(mis_class[0])/len(y)\n",
        "  return error\n"
      ],
      "metadata": {
        "id": "2ex7KvjmrCEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 1\n",
        "Define a function `gini_index(dict)` having the \n",
        "following characteristics:\n",
        "\n",
        "Input:\n",
        "`dict` = dictionary that has classes as keys and 'number of samples in respective class' as values of a particular node.\n",
        "\n",
        "Output:\n",
        "Gini index of the same node. (A float value)"
      ],
      "metadata": {
        "id": "dG6C1A_o4WTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def gini_index(dict):\n",
        "  n = sum(dict.values())\n",
        "  p_sum = 0\n",
        "  for value in duct.values():\n",
        "    p_sum += (value/n) * (value/n)\n",
        "  gini = 1- p_sum\n",
        "  return gini\n"
      ],
      "metadata": {
        "id": "-POE10604iHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 2\n",
        "Define a function `entropy(dict)` having the \n",
        "following characteristics:\n",
        "\n",
        "Input:\n",
        "`dict` = dictionary that has classes as keys and 'number of samples in respective class' as values of a particular node.\n",
        "\n",
        "Output:\n",
        "Entropy of the same node. (A float value)"
      ],
      "metadata": {
        "id": "gXatOtu6_mHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def entropy(dict):\n",
        "  n = sum(dict.values())\n",
        "  p = 0\n",
        "  for value in dict.values():\n",
        "    p += -(value/n)*np.log2(value/n)\n",
        "  return p"
      ],
      "metadata": {
        "id": "zNU4a2Xk7ggD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 3\n",
        "Consider a regression problem using CART. If y is a ndarray of targets of the samples in a particular node, define a function `sseloss(y)` that returns the error associated with that node."
      ],
      "metadata": {
        "id": "l2Y8opoNBX6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sseloss(y):\n",
        "  y_mean = np.mean(y)\n",
        "  return np.sum((y - y_mean)**2, axis = 0)"
      ],
      "metadata": {
        "id": "3h7cgfC8BdBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**WEEK_10**"
      ],
      "metadata": {
        "id": "fWis0Ea_CmYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 1\n",
        "Write a function `similarity(res,lambda_)` which takes residuals at a node as input and returns similarity score which can be used for `XGBoost regression.`\n",
        "\n",
        "Input: A numpy array having residuals at a node.\n",
        "          `lambda` is the regularization rate.\n",
        "\n",
        "Output: Similarity score of the node.\n",
        "\n",
        "`Note: Similarity Score= (Sum of residuals)2/(No. of residuals+lambda_)`"
      ],
      "metadata": {
        "id": "-mJWKFDhCxOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def similarity(res, lambda_):\n",
        "  s = np.sum(res)\n",
        "  d = len(res) + lambda_\n",
        "  return (s**2)/d"
      ],
      "metadata": {
        "id": "oqewdFotCop-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 2\n",
        "Write a function `gradboost(model,X_train, y_train, X_test, boosting_rounds,learning_rate)` to implement Gradient boost algorithm.\n",
        "\n",
        "Input:\n",
        "model: model to be fitted\n",
        "\n",
        "X_train: Training features\n",
        "\n",
        "y_train: Training output labels\n",
        "\n",
        "X_test: Test feature values\n",
        "\n",
        "boosting_rounds: number of boosting rounds\n",
        "\n",
        "learning_rate: learning rate used in the algorithm\n",
        "\n",
        "Output:\n",
        "\n",
        "y_hat_train: Prediction on training data after number of boosting rounds\n",
        "\n",
        "y_hat_test:  Prediction on testing data after number of boosting rounds"
      ],
      "metadata": {
        "id": "1vhwHcSXEIST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def GradBoost(model, X_train, y_train, X_test, boosting_rounds, learning_rate: float = 0.1):\n",
        "    #make a first guess of our training target variable using the mean of y_train\n",
        "    y_hat_train = np.repeat(np.mean(y_train), len(y_train))\n",
        "\n",
        "    #Initiate the test prediction with the mean of the training target variable.\n",
        "    y_hat_test = np.repeat(np.mean(y_train), len(y_test))\n",
        "\n",
        "    #Calculate the residuals\n",
        "    residuals = y_train - y_hat_train\n",
        "\n",
        "    #Iterate through the boosting rounds\n",
        "    for i in range(boosting_rounds):\n",
        "        #Fit the model to the residuals\n",
        "        model = model.fit(X_train, residuals)\n",
        "\n",
        "        #Increment the predicted training y with the pseudoresidulas * learning rate\n",
        "        y_hat_train = y_hat_train + learning_rate * model.predict(X_train)\n",
        "\n",
        "        #Increment the predicted test y as well\n",
        "        y_hat_test = y_hat_test + learning_rate * model.predict(X_test)\n",
        "\n",
        "        #Calculate the residuals for the next round\n",
        "        residuals = y_train - y_hat_train\n",
        "\n",
        "    return y_hat_train, y_hat_test"
      ],
      "metadata": {
        "id": "RxG4f7qoEJZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 1\n",
        "Write a function `reidual(y)` which takes a numpy array y as input and calculates residuals for base model(taking mean of the target values).\n",
        "\n",
        "Input: numpy array y having all the target values.\n",
        "\n",
        "Output: numpy array containing residuals."
      ],
      "metadata": {
        "id": "HojiBDsROTT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def residuals(y):\n",
        "  y_hat = np.repeat(np.mean(y), len(y))\n",
        "  res = y - y_hat"
      ],
      "metadata": {
        "id": "cL-f8e8DOOEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 2\n",
        "Write a function `accuracy(y_true,y_pred)` which calculates accuracy of classification based on inputs `y_true` and `y_pred.`\n",
        "\n",
        "Input:\n",
        "\n",
        "`y_true`- vector of true output labels\n",
        "\n",
        "`y_pred`-vector of predicted output labels.\n",
        "\n",
        "Output:\n",
        "\n",
        " accuracy of classification"
      ],
      "metadata": {
        "id": "6vrX1G0CPAon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  return np.sum(y_true == y_pred)/len(y_test)"
      ],
      "metadata": {
        "id": "4b5ojYcpO2My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 3\n",
        "Write a function `bag(X,y)`  for creating q bootstrap from original dataset.\n",
        "\n",
        "`Input:` X is the feature matrix and y is the output label vector\n",
        "\n",
        "`Output:` Bootstrap containing two numpy arrays containing feature values and corresponding target values."
      ],
      "metadata": {
        "id": "zyOzeOI8QZmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "def bag(X,y):\n",
        "  n = X.shape[0]\n",
        "  indices = np.random.choice(n, size = n, replace=True)\n",
        "  return X[indices], y[indices]"
      ],
      "metadata": {
        "id": "NNTZklAOQDTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**WEEK_11**"
      ],
      "metadata": {
        "id": "BxYAzQyfSJK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practice Programing Question 1\n",
        "Write a function `euclid(a,b)` to find Euclidean distance between vectors a and b.\n",
        "\n",
        "Input: Vectors a and b\n",
        "\n",
        "Return: Euclidean distance between vectors a and b\n",
        "\n",
        "Note: both vectors are of shape (m, ), where m is some positive integer."
      ],
      "metadata": {
        "id": "LrvkaFGsR5zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def euclid(a,b):\n",
        "  return np.sqrt(np.sum((a-b)**2),axis = 0)"
      ],
      "metadata": {
        "id": "0BQ2M7ONRfDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def euclid(a,b):\n",
        "    E = np.linalg.norm(a - b)\n",
        "    return E"
      ],
      "metadata": {
        "id": "_6833MWJRrXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 1\n",
        "Write a function to` centroid(a,b)` to find centroid of vectors a and b.\n",
        "\n",
        "Input: a and b are numpy arrays of same shape.\n",
        "\n",
        "Output: centroid of a and b as numpy array"
      ],
      "metadata": {
        "id": "hGVgyLQLTcNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def centroid(a, b):\n",
        "  data = (a, b)\n",
        "  c = np.mean(data, axis = 0)\n",
        "  return c"
      ],
      "metadata": {
        "id": "ZgE6KGIvTnJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def centroid(a, b):\n",
        "  n = a.shape[0]\n",
        "  c = []\n",
        "  for i in range(n):\n",
        "    c.append((a[i]+b[i])/2)\n",
        "  return np.array(c)"
      ],
      "metadata": {
        "id": "q0_-9Xd8UQ1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graded Programing Question 2\n",
        "Write a function `silhoutte(a,b)` to calculate silhoutte coefficient.\n",
        "\n",
        "Input: `a` is the mean distance between the instances in the cluster and `b` is the mean distance between the instance and the instances in the next closest cluster.\n",
        "\n",
        "Output: Silhoutte coefficient"
      ],
      "metadata": {
        "id": "4dHEFDQUUylS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def silhoutte(a,b):\n",
        "  return (a*b)/max(a,b)"
      ],
      "metadata": {
        "id": "spE-gtarU8o1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}